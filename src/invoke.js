// All functions that directly touch invoke should be handled and/or exported from here
// Abstract and simplify external interface as much as possible

const {config,log,debugLog,getUUID,validUUID,urlToBuffer,sleep,shuffle,tidyNumber}=require('./utils.js')
const {random}=require('./random.js')
const {exif}=require('./exif.js')
const io = require('socket.io-client')
const axios = require('axios')
const FormData = require('form-data')
var colors = require('colors')
const { isString, isObject } = require('lodash')
const parseArgs = require('minimist')
const {imageEdit} = require('./imageEdit.js')
var cluster=config.cluster

const init=async()=>{
    // Setup cluster of invoke ai backends starting with primary
    let initmsg=''
    let err=null
    for (const d in cluster){
    //cluster.forEach(async (c)=>{
        let c=cluster[d]
        try{
            if(c.disabled){break}//debugLog(c.name+' is disabled, skipping');
            initHost(c)
        } catch(err) {
            c.online=false
            initmsg+='Failed to initialize invoke server '+c.name+' at '+c.url+'\n'+err
        }
    }
    return initmsg
}

const initHost=async(host)=>{
    try{
        // on connect, get all the backend info we can in parallel
        const [version, models, lora, ti, vae, controlnet, cfg] = await Promise.all([getVersion(host),getModels(host,'main'),getModels(host,'lora'),getModels(host,'embedding'),getModels(host,'vae'),getModels(host,'controlnet'),getConfig(host)])
        host.version = version
        host.models = models
        host.lora = lora
        host.ti = ti
        host.vae = vae
        host.controlnet = controlnet
        host.config = cfg
        host.online = true
        host.activeJob = null
        host.socket = io(host.url,{path: '/ws/socket.io'})
        log('Connected to '.bgGreen.black+host.name.bgGreen+' with InvokeAI Version: '+host.version+'\nModels: '+host.models.length+',Loras: '+host.lora.length+', Embeddings: '+host.ti.length+', Vaes: '+host.vae.length+', Controlnets '+host.controlnet.length)
        queueStatus(host)
        subscribeQueue(host,'arty')
        // still unable to subscribe to websocket, no event messages happen after subscribe_queue emit ?
    } catch (err) {
        host.online = false
        log('Failed to init host '+host.name+' : '+err.code)
    }
}

buildWorkflowFromJob = (job)=>{
    let essentials = {}
    let keys = Object.keys(job)
    for (const i in keys){
        let key = keys[i]
        if(['prompt','strength','control','controlstart','controlstop','controlweight','ipamodel'].includes(key)){essentials[key] = job[key]}
    }
    let workflow = {
        name:"arty",
        author:"ausbitbank",
        description:"Images generated by discord bot ARTY",
        contact:"",
        tags:"",
        notes:essentials,
        exposedFields:[],
        meta:{version:"1.0.0"},
        nodes:[]
    }
    return JSON.stringify(workflow)
}

buildGraphFromJob = async(job)=>{ // Build new nodes graph based on job details
    let graph = {
        id: getUUID(),
        nodes:{},
        edges:[],
    }
    let data = []
    let lastid={unet:null,clip:null,vae:null,latents:null,noise:null,image:null,width:null,height:null,controlnet:null,mask:null,denoise_mask:null,width:null,height:null,metadata_accumulator:null,ip_adapter:null}
    let pipe = (fromnode,fromfield,tonode,tofield)=>{return {source:{node_id:fromnode,field:fromfield},destination:{node_id:tonode,field:tofield}}}
    let node = (type,params,edges)=>{
        let newid=getUUID()
        graph.nodes[newid]={}
        graph.nodes[newid].type=type
        graph.nodes[newid].id=newid
        if(type==='l2i'){ graph.nodes[newid].workflow=buildWorkflowFromJob(job)
        } else {
            graph.nodes[newid].workflow=null
        }
        Object.keys(params)?.forEach((k)=>{graph.nodes[newid][k]=params[k]})
        // by tracking and updating most recent used ids we can break the job into components easier
        if(['main_model_loader','sdxl_model_loader','sdxl_model_refiner_loader','lora_loader','sdxl_lora_loader'].includes(type)){lastid.unet=newid}
        if(['main_model_loader','sdxl_model_loader','clip_skip','lora_loader','sdxl_lora_loader'].includes(type)){lastid.clip=newid}
        if(['sdxl_model_loader','sdxl_refiner_model_loader','sdxl_lora_loader'].includes(type)){lastid.clip2=newid}
        if(['sdxl_model_loader','main_model_loader','vae_loader','denoise_latents'].includes(type)){lastid.vae=newid}
        if(['t2l','ttl','lscale','l2l','i2l','denoise_latents','lresize'].includes(type)){lastid.latents=newid}
        if(['noise'].includes(type)){lastid.noise=newid}
        if(['controlnet'].includes(type)){lastid.control=newid}
        if(['openpose_image_processor','l2i','face_mask_detection'].includes(type)){lastid.image=newid}
        if(['face_mask_detection'].includes(type)){lastid.mask=newid,lastid.width=newid;lastid.height=newid}
        if(['create_denoise_mask'].includes(type)){lastid.denoise_mask=newid}
        if(['metadata_accumulator'].includes(type)){lastid.metadata_accumulator=newid}
        if(['ip_adapter'].includes(type)){lastid.ip_adapter=newid}
        edges?.forEach(e=>{
            if(!validUUID(e.destination.node_id)){ // not already plumbed with a valid UUID
                if(e.destination.node_id==='SELF'){ e.destination.node_id=newid
                }else{
                    let nodenumber=e.destination.node_id.split('-')[1]?e.destination.node_id.split('-')[1]:0
                    let nodetype=e.destination.node_id.split('-')[0]
                    let i=0
                    Object.keys(graph.nodes).forEach(n=>{
                        if(graph.nodes[n].type===nodetype){
                            if(i===nodenumber){e.destination.node_id=graph.nodes[n].node_id}
                            i++
                        }
                    })
                }
            }
            if(!validUUID(e.source.node_id)){ // not already plumbed with a valid UUID
                if(e.source.node_id==='SELF'){ e.source.node_id=newid
                }else{
                    let nodenumber=e.source.node_id.split('-')[1]?parseInt(e.source.node_id.split('-')[1]):0
                    let nodetype=e.source.node_id.split('-')[0]
                    let i=0
                    Object.keys(graph.nodes).forEach(n=>{
                        if(graph.nodes[n].type===nodetype){
                            if(i===(Math.max(nodenumber-1,0))){e.source.node_id=graph.nodes[n].id}
                            i++
                        }
                    })
                }
            }
            graph.edges.push(e)
        })
    }

    // Actual graph building starts here
    //      node(type,{parameters},[pipes])
    //      pipes flow backwards only to already created nodes eg
    //      pipe(from id, from field, to id, to field)
    //      id can either be actual id, or a type (type-number for multiples) or reference lastid.clip etc for most recent id's
    var p=[] // 
    // Metadata accumulator
    // todo add vae object and controlnets array
    let metaObject = {
        is_intermediate:false,
        generation_mode:job.initimg?'img2img':'txt2img',
        cfg_scale:job.scale,
        clip_skip:job.clipskip,
        height:job.height,
        width:job.width,
        positive_prompt:job.positive_prompt,
        negative_prompt:job.negative_prompt,
        rand_device:'cpu',
        scheduler:job.scheduler,
        steps:job.steps,
        model:job.model,
        loras:[],
        controlnets:[],
        vae:{model_name:'sd-vae-ft-mse',base_model:'sd-1'},
        positive_style_prompt:job.style??'',
        negative_style_prompt:job.negstyle??'',
        refiner_model:job.refiner_model,
        refiner_cfg_scale:job.refiner_scale,
        refiner_steps:job.refiner_steps,
        refiner_scheduler:null,
        refiner_positive_aesthetic_score:null,
        refiner_negative_aesthetic_score:null,
        refiner_start:null
    }
    // Reformat lora array for metadata object
    if(job.loras?.length>0){for (const l in job.loras){metaObject.loras.push({lora:{model_name:job.loras[l].model.model_name,base_model:job.loras[l].model.base_model},weight:job.loras[l].weight})}}
    if(job.control&&job.initimgObject){
        //metaObject.controlnets.push({controlnet:{model_name:job.control}})
    }
    if(['sd-1','sd-2'].includes(job.model.base_model)){
        node('main_model_loader',{model:job.model,is_intermediate:true},[])
        node('vae_loader',{vae_model:{model_name:'sd-vae-ft-mse',base_model:'sd-1'},is_intermediate:true},[])
        if(job.loras?.length>0){for (const l in job.loras) {node('lora_loader',{is_intermediate:true,lora:{base_model:job.loras[l].model.base_model,model_name:job.loras[l].model.model_name},weight:job.loras[l].weight},[pipe(lastid.clip,'clip','SELF','clip'),pipe(lastid.unet,'unet','SELF','unet')])}} // lora loader, chain multiple loras with clip and unet into each other
    } else {
        node('sdxl_model_loader',{model:job.model,is_intermediate:true},[])
        if(job.loras?.length>0){for (const l in job.loras) {node('sdxl_lora_loader',{is_intermediate:true,lora:{base_model:job.loras[l].model.base_model,model_name:job.loras[l].model.model_name},weight:job.loras[l].weight},[pipe(lastid.clip,'clip','SELF','clip'),pipe(lastid.clip2,'clip2','SELF','clip2'),pipe(lastid.unet,'unet','SELF','unet')])}} // lora loader, chain multiple loras with clip and unet into each other
    }

    if(job.initimgObject){
        debugLog('Adding init img to graph')
        if(job.control==='ipa'){ // todo rework so it can be used independantly of controlnet or i2l , allow model selection
            let ipamodel=(job.model.base_model==='sdxl')?'ip_adapter_sdxl':'ip_adapter_sd15'
            if(job.ipamodel){ipamodel=job.ipamodel}
            debugLog('Using ip_adapter with input image, model '+ipamodel)
            node('ip_adapter',{ip_adapter_model:{base_model:job.model.base_model,model_name:ipamodel},begin_step_percent:0,end_step_percent:1,is_intermediate:true,image:{image_name:job.initimgObject.image_name},weight:1},[])
        } else if(job.facemask){
            debugLog('Using face mask detection')
            node('face_mask_detection',{is_intermediate:true,face_ids:'0',minimum_confidence:0.5,x_offset:0,y_offset:0,chunk:false,invert_mask:job.invert??false,image:{image_name:job.initimgObject.image_name}})
            node('i2l',{is_intermediate:false,fp32:true},[pipe(lastid.vae,'vae','SELF','vae'),pipe(lastid.image,'image','SELF','image')])
            node('create_denoise_mask',{is_intermediate:false,fp32:true,tiled:false},[pipe(lastid.image,'image','SELF','image'),pipe(lastid.mask,'mask','SELF','mask'),pipe(lastid.vae,'vae','SELF','vae')])
            // todo lresize here ?
        } else if(job.control==='i2l'){
            // default to image to latents
            node('i2l',{is_intermediate:false,fp32:true,image:{image_name:job.initimgObject.image_name}},[pipe(lastid.vae,'vae','SELF','vae')])
            node('lresize',{model:'bilinear',antialias:false,width:job.width,height:job.height},[pipe(lastid.latents,'latents','SELF','latents')])
        } else {
            debugLog('Using controlnet '+job.control||'depth')
            node('controlnet',{image:{image_name:job.initimgObject.image_name},
                    control_model:{model_name:job.control?job.control:'depth',base_model:job.model.base_model},
                    control_weight:job.controlweight?job.controlweight:1,
                    begin_step_percent:job.controlstart?job.controlstart:0,
                    end_step_percent:job.controlend?job.controlend:1,
                    control_mode:job.controlmode?job.controlmode:'balanced',
                    resize_mode:job.controlresize?job.controlresize:'just_resize',
                    is_intermediate:true
                    },[])
        }
    }
    node('clip_skip',{skipped_layers:job.clipskip??0,is_intermediate:true},[pipe(lastid.clip,'clip','SELF','clip')])
    if(lastid.width&&lastid.height){
        node('noise',{use_cpu:true},[pipe(lastid.width,'width','SELF','width'),pipe(lastid.height,'height','SELF','height')])
    }else{
        node('noise',{width:job.width,height:job.height,use_cpu:true},[])
    }
    node('metadata_accumulator',metaObject,[])
    if(['sd-1','sd-2'].includes(job.model.base_model)){
        node('compel',{prompt:job.positive_prompt},[pipe(lastid.clip,'clip','SELF','clip')])
        node('compel',{prompt:job.negative_prompt},[pipe(lastid.clip,'clip','SELF','clip')])
        p = [
            pipe('compel','conditioning','SELF','positive_conditioning'),
            pipe('compel-2','conditioning','SELF','negative_conditioning'),
            pipe(lastid.noise,'noise','SELF','noise'),
            pipe(lastid.unet,'unet','SELF','unet')
        ]
    } else {
        node('sdxl_compel_prompt',{prompt:job.positive_prompt,original_width:job.width??1024,original_height:job.height??1024,crop_top:0,crop_left:0,target_width:job.width??1024,target_height:job.height??1024,style:job.style??'',is_intermediate:true},[pipe(lastid.clip,'clip','SELF','clip'),pipe(lastid.clip2,'clip2','SELF','clip2')])
        node('sdxl_compel_prompt',{prompt:job.negative_prompt,original_width:job.width??1024,original_height:job.height??1024,crop_top:0,crop_left:0,target_width:job.width??1024,target_height:job.height??1024,style:job.negstyle??'',is_intermediate:true},[pipe(lastid.clip,'clip','SELF','clip'),pipe(lastid.clip2,'clip2','SELF','clip2')])
        p = [
            pipe('sdxl_compel_prompt','conditioning','SELF','positive_conditioning'),
            pipe('sdxl_compel_prompt-2','conditioning','SELF','negative_conditioning'),
            pipe(lastid.noise,'noise','SELF','noise'),
            pipe(lastid.unet,'unet','SELF','unet')
        ]
    }
    if(lastid.control){p.push(pipe(lastid.control,'control','SELF','control'))}
    if(lastid.ip_adapter){p.push(pipe(lastid.ip_adapter,'ip_adapter','SELF','ip_adapter'))}
    if(lastid.denoise_mask){p.push(pipe(lastid.denoise_mask,'denoise_mask','SELF','denoise_mask'))}
    if(lastid.latents){p.push(pipe(lastid.latents,'latents','SELF','latents'))}
    let denoising_start = 0.0
    if(job.strength&&job.initimgObject&&job.control==='i2l'){denoising_start=1.0-job.strength}
    node('denoise_latents',{is_intermediate:true,noise:null,steps:job.steps,cfg_scale:job.scale,denoising_start:denoising_start,denoising_end:1.0,scheduler:job.scheduler},p)

    if(job.lscale&&job.lscale!==1){ // upscale latents, low fidelity
        node('lscale',{scale_factor:job.lscale,mode:'bilinear',antialias:false},[pipe(lastid.latents,'latents','SELF','latents')])
        node('noise',{use_cpu:true},[pipe('lscale','width','SELF','width'),pipe('lscale','height','SELF','height')])
        let p=[pipe('compel','conditioning','SELF','positive_conditioning'),pipe('compel-2','conditioning','SELF','negative_conditioning'),pipe(lastid.noise,'noise','SELF','noise'),pipe(lastid.unet,'unet','SELF','unet'),pipe(lastid.latents,'latents','SELF','latents')]
        if(lastid.control){p.push(pipe(lastid.control,'control','SELF','control'))}
        node('denoise_latents',{is_intermediate:true,noise:null,steps:job.steps,cfg_scale:job.scale,denoising_start:denoising_start,denoising_end:1.0,scheduler:job.scheduler},p)
    }
    if(['sd-1','sd-2'].includes(job.model.base_model)){
        node('l2i',{tiled:true,fp32:true,is_intermediate:true},[pipe('vae_loader','vae','SELF','vae'),pipe(lastid.latents,'latents','SELF','latents'),pipe('metadata_accumulator','metadata','SELF','metadata')])
    } else {
        node('l2i',{tiled:true,fp32:true,is_intermediate:true},[pipe('sdxl_model_loader','vae','SELF','vae'),pipe(lastid.latents,'latents','SELF','latents'),pipe('metadata_accumulator','metadata','SELF','metadata')])
    }
    if(job.upscale&&job.upscale===2){node('esrgan',{model_name:'RealESRGAN_x2plus.pth'},[pipe(lastid.image,'image','SELF','image')])}
    //debugLog('Add data section to graph. Initial seed is '+job.seed+' , number is '+job.number)
    let dataitems = [job.seed]
    while(dataitems.length<job.number){dataitems.push(random.seed())}
    data.push([
        {node_path:lastid.noise,field_name:'seed',items:dataitems},
        {node_path:lastid.metadata_accumulator,field_name:'seed',items:dataitems}
    ])
    // Tada! Graph built, submit to backend
    debugLog(graph)
    return {
        prepend:false,
        batch:{
            data:data,
            graph:graph,
            runs:1,
        }
    }
}

const enqueueBatch = async (host, graph, name='arty') => {
    // new in invoke 3.2.0rc1
    try {
        const response = await axios.post(host.url + '/api/v1/queue/'+name+'/enqueue_batch',graph)
        return response.data.batch.batch_id
    } catch (err) {
        console.error('Error queueing batch',err.data)
        throw('Error queueing batch',+err.code)
    }
}


const batchStatus = async (host,batch_id,name='arty')=>{
    try {
        const response = await axios.get(host.url + '/api/v1/queue/'+name+'/b/'+batch_id+'/status')
        return response.data
    } catch (err) {
        log(err)
        throw(err.code)
    }
}


const queueStatus = async (host,name='arty')=>{
    try {
        const response = await axios.get(host.url + '/api/v1/queue/'+name+'/status')
        if(response.data.queue.pending===0&&response.data.queue.in_progress===0&&response.data.queue.total>10){queuePrune(host,name)}
        return response.data
    } catch (err) {
        log(err)
        throw(err.code)
    }
}

const queueList = async (host,name='arty')=>{
    try {
        const response = await axios.get(host.url + '/api/v1/queue/'+name+'/list')
        return response.data
    } catch (err) {
        log(err)
        throw(err.code)
    }
}

const queuePrune = async (host,name='arty')=>{
    try {
        const response = await axios.put(host.url + '/api/v1/queue/'+name+'/prune')
        log('Pruning queue "'+name+'" on host '+host.name+'; deleted '+response.data?.deleted+' completed or failed jobs.')
        return response.data
    } catch (err) {
        log(err)
        throw(err.code)
    }
}


const pollQueue = async (host,id) => {
    let ms = 1000
    let err = false
    let qsq = null
    let sessionids = []
    while(!err){
        try{
            log('queuestatus '+host.name)
            let qs = await queueStatus(host)
            if(qsq!==qs.queue){
                log(qs.queue)
                qsq=qs.queue
            }
            let q = await queueList(host)
            if(q.items?.length>0){
                for (const qi in q.items){
                    let i = q.items[qi]
                    if(i.batch_id===id){
                        log(i.batch_id+' is '+i.status)
                        if(['failed','canceled'].includes(i.status)){err=true}
                        if(['completed'].includes(i.status)){
                            debugLog('completed batch '+i.batch_id+' claiming session id(s) '+sessionids)
                            return i.session_id
                        }
                        if(i.status==='in_progress'&&i.session_id){
                            debugLog('in progress batch '+i.batch_id+' claiming session id(s) '+sessionids)
                            return i.session_id
                        }
                    }
                }
            }
            await sleep(ms)
        } catch(err) {log(err)}
    }
}

/*
Disabled from invoke 3.2rc
const postSession = async (host, graph) => {
    try {
        const response = await axios.post(host.url + '/api/v1/sessions/', graph)
        return response.data.id
    } catch (err) {
        console.error('Error posting session', err.data)
        throw('Error posting session: '+err.code)
    }
}

const cancelSession = async(host,id) => {
    try {
        let u=host.url+'/api/v1/sessions/'+id+'/invoke'
        debugLog('cancel session '+id+' on '+host.name)
        let response = axios.delete(u)
    } catch (e) {
        log(e.response.statusText)
        throw(e.response.statusText)
    }
}

const startSession = async(host,id)=>{
    try {
        let u=host.url+'/api/v1/sessions/'+id+'/invoke?all=true'
        let response = axios.put(u)
        return response
    } catch (e) {
        log(e.response.statusText)
        throw('Error starting session: '+e.response.statusText)
    }
}
*/
const findHost = async(job=null)=>{
    // find host with the required models, embeds, etc that isn't currently busy
    let availableHosts=cluster.filter(h=>{return h.online&&!h.disabled})
    //if(job?.host){return cluster[job.host]}
    if(job===null&&availableHosts.length>0){
        debugLog('No job info supplied, returning random available host')
        return availableHosts[0]
    }
    if(isString(job?.model)){
        debugLog('Job.model is a string, convert to model object')
        try{ job.model=await modelnameToObject(job.model)
        }catch(err){
            log('error in findHost model search')
            log(err)
            throw(err)
        }
    }
    let filteredHosts = availableHosts.filter(host => {return host.models.some(model => model.name === job.model.name)})
    // todo sort online hosts by priority value
    if(filteredHosts.length > 0) {
        return filteredHosts[Math.floor(Math.random() * filteredHosts.length)]
    } else {
        throw('No host with required model found') 
    }
}

async function checkOfflineHosts() {
    for(let host of cluster) {
        if(!host.online) {
            try {
                await getVersion(host)
                log('Host '+host.name+' just came online - reenabled')
                host.online = true
            } catch {
            // continue being offline
            }
        }
    }
}

function handleError(err) {
    host.online = false
    console.error(err)
    throw err
}

const subscribeQueue = async(host,name='arty')=>{
    let socket = host.socket
    try{
        socket.on('connect',()=>{
            log('websocket connected on '+host.url)
            log('subscribequeue on host '+host.name+' : '+name)
            socket.emit('subscribe_queue',{queue_id:name},()=>{log('emitted subscribe_queue')})
        })
        socket.on('message', msg => {log('websocket message:');log(msg)})
        socket.on('event', msg => {log('websocket event:');log(msg)})
    } catch(err) {
        log('Error wth websocket for host '+host.name)
    }
}



const pollSession = async(host,id)=>{
    // I hate this solution, but it works for now.
    // Ideally we should subscribe to session via websocket
    let ms=1000
    let err=false
    while(!err) {
        try{
            let sesh = await getSession(host,id)
            if(Object.keys(sesh.errors).length>0){err=true;throw(sesh.errors)}
            if(isSessionComplete(sesh,host.name)){ return sesh
            } else {
                await sleep(ms)
                ms=ms*1.10 // increase poll interval by 10% each time
                if(ms>=5000){ms=1000} // reset once we hit 5 second polling
            }
        }catch(e){
            log(e)
            //host.online = false
            err=true
        }
    }
}


const getSessionStats = (session)=>{
    let stats = {
        id:session.id,
        results:session.results
    }
    for (const r in stats.results){
        log(stats.results[r])
    }
}

// a tmp cache to reduce redundant progress logs, remove when we update to websocket progress subscriptions
let isctmp = [] 

const isSessionComplete = (session,hostname)=>{
    // return true/false eventually, use debug logging for progress updates
    try{
        isctmpnew = hostname+' '+session.id+' : '+Object.keys(session.results).length+' / '+Object.keys(session.execution_graph.nodes).length
        if(!isctmp.includes(isctmpnew)){ // check if we've already posted this message recently
            debugLog(isctmpnew)
            isctmp.push(isctmpnew) // add message to cache
            if(isctmp.length>10){isctmp.slice(-10)} // maximum of 10 in cache, slice to size
        }
        if(Object.keys(session.results).length===Object.keys(session.execution_graph.nodes).length){return true
        }else{return false}
    } catch(err) {
        throw(err)
    }
}

getSessionImages = async(host,session)=>{
    // return an array of image objects from session results
    try{
        debugLog(host.name+' '+session.id+' getSessionImages')
        let ia=[]
        let results=[]
        for (const r of Object.keys(session.results)){
            let result=session.results[r]
            if(result.type==='image_output'){ia.push(result)} // &&result.is_intermediate!==true
        }
        for (const i of ia){
            i.name=i.image.image_name
            i.buffer = await getImageBuffer(host,i.name)
            if(i.buffer?.error){return {error:i.buffer.error}}
            if(host.deleteAfterRender)deleteImage(host,i.name)
            results.push(i)
        }
        if(results.length===0){
            log('No images found in session')
        }
        return results
    } catch(err){
        // host.online = false
        log(err)
    }
}

const deleteImage = async(host,name)=>{
    try{
        u = host.url+'/api/v1/images/i/'+name
        debugLog('Deleting image '+name+' from '+host.name)
        await axios.delete(u)
    } catch (err) {
        host.online = false
        throw(err)
    }
}

const subscribe = async (socket, host, id) => {
    return new Promise((resolve, reject) => {
        socket.on('connect', () => {
            console.log('Connected to socket')
        })
        socket.on('message', msg => {console.log('Socket message:', msg)})
        socket.on('sid', sid => {console.log('sid:', sid)})
        socket.on('error', err => {
            console.error('Socket error:', err)
            reject(err)
        })
        socket.emit('subscribe', {session: id}, (result) => {
            socket.on('message', (msg) => {
                if(msg.session === id) {
                    resolve(msg)
                }
            })
        })
    })
}

const getVersion = async(host)=>{
    try{
        let u = host.url+'/api/v1/app/version'
        let response = await axios.get(u)
        return response.data.version
    } catch (err){
        host.online = false
        throw(err)
    }
}

const getConfig = async(host)=>{
    try{
        let u = host.url+'/api/v1/app/config'
        let response = await axios.get(u)
        return response.data
    } catch (err){
        host.online = false
        throw(err)
    }
}

const getModels = async(host,type='main')=>{
    // type can be embedding , main , vae , controlnet , lora
    try {
        let u = host.url+'/api/v1/models/?model_type='+type
        let response = await axios.get(u)
        return response.data?.models
    } catch (err) {
        host.online = false
        throw(err)
    }
}

const getSessions = async(host)=>{
    try {
        let u = host.url+'/api/v1/sessions/'
        let response = await axios.get(u)
        return r.data?.items
    } catch (err) {
        host.online = false
        throw(err)
    }
}

// in invoke 3.2rc all /sessions api endpoints are deprecated
const getSession = async(host,id)=>{
    try {
        let u = host.url + '/api/v1/sessions/' + id
        const response = await axios.get(u)
        return response.data
    } catch (err) {
        console.error('Error getting session', err)
        host.online = false
        throw err
    } 
}

const getImages = async(host)=>{
    try {
        let u = host.url+'/api/v1/images/?board_id=none&categories=control&categories=mask&categories=user&categories=other&is_intermediate=false&limit=0&offset=0'
        let response = axios.get(u)
        return r.data?.items
    } catch (err) {
        host.online = false
        throw(err)
    }
}

const getImage = async(host,name)=>{
    try {
        let u = host.url+'/api/v1/images/'+name
        let r = axios.get(u)
        return r.data
    } catch (err) {
        host.online = false
        throw(err)
    }
}

const getImageBuffer = async(host,name)=>{
    try {
        let u = host.url+'/api/v1/images/i/'+name+'/full'
        let buf = urlToBuffer(u)
        return buf
    } catch (err) {
        host.online = false
        throw(err)
    }
}

getHeaders=(form)=>{
    form.getLength((err, length) => {
        if(err){throw(err)}
        let headers=Object.assign({'Content-Length': length}, form.getHeaders())
        return headers
    })
}

uploadInitImage=async(host,buf,id)=>{
    try{
        debugLog('Uploading init img to '+host.name+' with id '+id)
        let form = new FormData()
        form.append('data',JSON.stringify({kind:'init'}))
        form.append('file',buf,{contentType:'image/png',filename:id+'.png'})
        let headers = await getHeaders(form)
        let url=host.url+'/api/v1/images/upload?image_category=user&is_intermediate=false'
        let response = await axios.post(url,form,{headers:headers})
        return response.data
    } catch (err) {
        host.online = false
        throw(err.code)
    }
}

const auto2invoke = (text)=>{
  // convert auto1111 weight syntax to invokeai
  // todo convert lora syntax eg <lora:add_detail:1> to withLora(add_detail,1)
  const regex = /\(([^)]+):([^)]+)\)/g
  return text.replaceAll(regex, function(match, $1, $2) {
    return '('+$1+')' + $2
  })
}

const jobFromDream = async(cmd,img=null)=>{
    // input oldschool !dream format, output job object
    debugLog('jobfromdream - cmd:'+cmd)
    var job = parseArgs(cmd,{boolean:['facemask','invert']})//string: ['sampler','text_mask'],boolean: ['seamless','hires_fix']}) // parse arguments //
    // set argument aliases
    if(job.s){job.steps=job.s;delete job.s}
    if(job.S){job.seed=job.S;delete job.S}
    if(job.W){job.width=job.W;delete job.W}
    if(job.H){job.height=job.H;delete job.H}
    if(job.C){job.scale=job.C;delete job.C}
    if(job.A){job.sampler=job.A;delete job.A}
    if(job.f){job.strength=job.f;delete job.f}
    if(job.hrf){job.hires_fix=job.hrf;delete job.hrf}
    if(job.n){job.number=job.n;delete job.n}
    if(job.sampler){job.scheduler=job.sampler;delete job.sampler}
    // take prompt from what's left
    debugLog(job._)
    job.prompt=job._.join(' ')
    if(img){job.initimg=img}
    return validateJob(job)
}

const jobFromMeta = async(meta,img=null)=>{
    let job = {}
    if(meta.invoke?.prompt){
        job.prompt=meta.invoke?.prompt
    }else if(meta.invoke?.positive_prompt && meta.invoke?.negative_prompt){
        job.prompt = meta.invoke.positive_prompt+'['+meta.invoke.negative_prompt+']'
    }
    if(meta.invoke?.style){job.style=meta.invoke.style}
    if(meta.invoke?.negstyle){job.style=meta.invoke.negstyle}
    if(img){job.initimg=img}
    if(meta.invoke?.inputImageUrl){job.initimg=await urlToBuffer(meta.invoke?.inputImageUrl)}else{job.initimg=null}
    if(meta.invoke?.control){job.control=meta.invoke.control}
    if(meta.invoke?.controlstart){job.controlstart=meta.invoke.controlstart}
    if(meta.invoke?.controlend){job.controlend=meta.invoke.controlend}
    if(meta.invoke?.controlweight){job.controlweight=meta.invoke.controlweight}
    job.steps = meta.invoke?.steps ? meta.invoke.steps : config.default.steps
    // todo need to look at job.model.base_model and use sdxl width/height defaults if not already in meta.invoke
    job.width = meta.invoke?.width ? meta.invoke.width : config.default.width ?? config.default.size
    job.height = meta.invoke?.height ? meta.invoke.height : config.default.height ?? config.default.size
    job.scheduler = meta.invoke?.scheduler ? meta.invoke.scheduler : config.default.scheduler
    job.loras = meta.invoke?.loras ? meta.invoke.loras : []
    job.scale = meta.invoke?.scale ? meta.invoke.scale : config.default.scale
    job.model = meta.invoke?.model ? meta.invoke.model : config.default.model
    return validateJob(job)
}

const jobFromGraph = async(graph)=>{
    // input invokeai graph taken from image metadata, output job object
}

const getDiffusionResolution = (number)=>{
    // Diffusion resolution needs to be divisible by a specific number
    // invoke2 = 64 , invoke3 = 8
    let smallestResStep = 8
    const quotient = Math.floor(number / smallestResStep)  // Get the quotient of the division
    const closestNumber = quotient * smallestResStep  // Multiply the quotient by res step to get the closest number
    return closestNumber
}

const extractLoras = async (inputString) => {
    const loraRegex = /withLora\s*\(([^,]+?)(?:,(\d+(?:\.\d+)?))?\)/gi
    const matches = [...inputString.matchAll(loraRegex)]
    //if(!matches.length) {return {error: "No loras found"}}
    const loras = matches.map(match => {
        const name = match[1]
        let weight
        let m=match[0]
        if(match[2]) {weight = match[2]} else {weight = 0.85}
        return {
            name, 
            weight,
            m
        }
    })
    let stripped = inputString
    let newloras = []
    for (const lora of loras){
        lora.model = await loranameToObject(lora.name)
        if(!lora.model) break
        stripped = inputString.replace(lora.m, '')
        newloras.push({name:lora.name,model:lora.model,weight:lora.weight})
    }
    return {
        loras: newloras,
        stripped: stripped,
        inputString: inputString
    }
}

const allUniqueModelsAvailable = async()=>{
    // Return an object with all available models, across all connected & online hosts //todo : no repeats
    let availableHosts=cluster.filter(h=>{return h.online&&!h.disabled})
    let allModels=[]
    for (const h in availableHosts){
        let host=cluster[h]
        for (const m in host.models){
            let model = host.models[m]
            allModels.push({model_name:model.model_name,base_model:model.base_model,model_type:model.model_type,description:model.description})
        }
    }
    return allModels
}
/*
    // look up models available on hosts
    let availableHosts=cluster.filter(h=>{return h.online&&!h.disabled})
    for (const h in availableHosts){
        let host=cluster[h]
        let model=host.models.find(m=>{return m.model_name===modelname})
        if(isObject(model)){
            return {
                model_name: model.model_name,
                base_model: model.base_model,
                model_type: model.model_type,
                description: model.description
            }
*/

const validateJob = async(job)=>{
    // examine job object, reject on invalid parameters, add defaults as required
    // if no prompt, get a random one
    if(!job.prompt||job.prompt.length===0) job.prompt=random.get('prompt')
    // replace randomisers
    job.prompt=random.parse(job.prompt)
    // convert prompt weighting from auto1111 format to invoke/compel
    job.prompt=auto2invoke(job.prompt)
    // extract Loras in withLora(name,wieght) format into job.loras
    try{
        let el = await extractLoras(job.prompt)
        if(el.error){return {error:el.error}}
        //job.prompt = el.stripped // Decided against stripping from prompt
        job.loras = el.loras
    } catch(err){
        return {error: err}
    }
    // split into positive/negative prompts
    const npromptregex = /\[(.*?)\]/g // match content of [square brackets]
    const npromptmatches = job.prompt?.match(npromptregex)
    if(npromptmatches?.length>0){job.negative_prompt=npromptmatches.join(' ').replace('[','').replace(']','')
    }else{job.negative_prompt='<neg-sketch-3>,blur'}
    job.positive_prompt=job.prompt?.replace(npromptregex,'')
    // set defaults if not already set
    if(!job.style){job.style=''}
    if(!job.negstyle){job.negstyle=''}
    if(!job.number){job.number=1}else if(job.number>1&&job.seed){delete job.seed} // cannot feed a seed into the iterator afaik
    if(!job.seed&&job.number!==1||!Number.isInteger(job.seed)||job.seed<1||job.seed>4294967295){job.seed=random.seed()}
    if(!job.model){job.model=await modelnameToObject(config.default.model)}//{model_name:'degenerate526urpm',base_model:'sd-1',model_type:'main'}}
    if(!isObject(job.model)){job.model=await modelnameToObject(job.model)}
    if(!job.steps){job.steps=config.defaultSteps? config.defaultSteps : 30}
    if(job.steps>config.maximum.steps){return{error:'Steps `'+job.steps+'` is above the current maximum step count `'+config.maximum.steps+'`'}}
    if(job.model?.base_model==='sdxl'){
        if(!job.width){job.width=config.default.sdxlwidth??1024}
        if(!job.height){job.height=config.default.sdxlheight??1024}
    } else {
        if(!job.width){job.width=config.default.width??512}
        if(!job.height){job.height=config.default.height??512}
    }
    job.height=getDiffusionResolution(job.height)
    job.width=getDiffusionResolution(job.width)
    if(config.maximum.pixels&&(job.width*job.height)>config.maximum.pixels){
        let error = 'Width `'+job.width+'` x Height `'+job.height+'` = `'+tidyNumber(job.width*job.height)+'` , above the current maximum pixel count of `'+tidyNumber(config.maximum.pixels)+'`'
        debugLog(error)
        return {error:error}
        //job.width=config.default.size?config.default.size:512
        //job.height=config.default.size?config.default.size:512
    }
    if(!job.scale){job.scale=config.default.scale? config.default.scale : 0.7}
    // scheduler must be one of these
    let validSchedulers=config.schedulers||['ddim','ddpm','deis','lms','lms_k','pndm','heun','heun_k','euler','euler_k','euler_a','kdpm_2','kdpm_2_a','dpmpp_2s','dpmpp_2s_k','dpmpp_2m','dpmpp_2m_k','dpmpp_2m_sde','dpmpp_2m_sde_k','dpmpp_sde','dpmpp_sde_k','unipc']
    if(!job.scheduler){job.scheduler=config.default.scheduler? config.default.scheduler : 'dpmpp_2m_sde_k'}
     // lscale min 1 max 3 default 1
    if(!job.lscale||job.lscale<1||job.lscale>3){job.lscale=1}
    if(!job.clipskip){job.clipskip=0}
    if(!job.upscale){job.upscale=0}
    // set default init img mode
    if(!job.control&&job.initimg){job.control='i2l'}
    if(job.controlresize&&['just_resize','crop_resize','fill_resize'].includes(job.controlresize)===false){job.controlresize='just_resize'}
    if(job.controlmode&&['balanced','more_prompt','more_control','unbalanced'].includes(job.controlresize)===false){job.controlresize='just_resize'}
    //debugLog(job)
    return cast(job)
}

const rawGraphResponse = async(host,graph)=>{
    let id = await postSession(host,graph)
    await startSession(host,id)
    let session = await pollSession(host,id)
    //debugLog(session)
    return session
}

const auditGraph = async(job)=>{
    // audit a completed job graph for cost/time
}

const extractMetaFromSession = async(session)=>{
}

const hostHasModel = async(host,model)=>{
    if(host?.models?.includes(model)){ return true
    } else { return false}
}

const depthMap = async(img,host,a_mult=2,bg_th=0.1)=>{
    try{
        if(!host)host=await findHost()
        let imgid = getUUID()
        let initimg = await uploadInitImage(host,img,imgid)
        let graph={'nodes':{'midas_depth_image_processor':{'id':'midas_depth_image_processor','type':'midas_depth_image_processor','a_mult':a_mult,'bg_th':bg_th,'is_intermediate':false,'image':{'image_name':initimg.image_name}}}}
        let session = await rawGraphResponse(host,graph)
        let images = await getSessionImages(host,session)
        deleteImage(host,initimg.image_name)
        return {images: images}
    } catch (err) {
        log(err)
        return {error: err}
    }
}

const canny = async(img,host,low_threshold=100,high_threshold=200)=>{
    try{
        if(!host)host=await findHost()
        let imgid = getUUID()
        let initimg = await uploadInitImage(host,img,imgid)
        let graph={'nodes':{'canny_image_processor':{'id':'canny_image_processor','type':'canny_image_processor','low_threshold':low_threshold,'high_threshold':high_threshold,'is_intermediate':false,'image':{'image_name':initimg.image_name}}}}
        let session = await rawGraphResponse(host,graph)
        let images = await getSessionImages(host,session)
        deleteImage(host,initimg.image_name)
        return {images: images}
    } catch (err) {
        log(err)
        return {error: err}
    }
}

const openpose = async(img,host,detect_resolution=512,hand_and_face=true,image_resolution=512)=>{
    try{
        if(!host)host=await findHost()
        let imgid = getUUID()
        let initimg = await uploadInitImage(host,img,imgid)
        let graph={'nodes':{'openpose_image_processor':{'id':'openpose_image_processor','type':'openpose_image_processor','detect_resolution':detect_resolution,'hand_and_face':hand_and_face,'image_resolution':image_resolution,'is_intermediate':false,'image':{'image_name':initimg.image_name}}}}
        let session = await rawGraphResponse(host,graph)
        let images = await getSessionImages(host,session)
        deleteImage(host,initimg.image_name)
        return {images: images}
    } catch (err) {
        log(err)
        return {error: err}
    }
}

const esrgan = async(img,host,model_name='RealESRGAN_x2plus.pth')=>{
    try{
        if(!host)host=await findHost()
        let imgid = getUUID()
        // read image dimensions before upscale
        let resolution = await imageEdit.getResolution(img)
        let width = resolution?.width
        let height = resolution?.width
        let totalPixels = width * height
        // use config.maximum.pixels to find limits
        let maxPixels = config.maximum.upscaledPixels ?? 4194304
        if(totalPixels>=maxPixels){return {error: 'Image dimensions are too large! Max upscaled pixels = '+maxPixels}}
        let initimg = await uploadInitImage(host,img,imgid)
        let graph={
            prepend:false,
            batch:{
                data:[],
                graph:{
                    nodes:{esrgan:{'id':'esrgan','type':'esrgan','model_name':model_name,'image':{'image_name':initimg.image_name}}}
                },
                runs:1
            }
        }
        let batchId = await enqueueBatch(host,graph)
        let sessionId = await pollQueue(host,batchId)
        let session = await pollSession(host,sessionId) 
        let images = await getSessionImages(host,session)
        deleteImage(host,initimg.image_name)
        return {images: images}
    } catch (err) {
        log(err)
        return {error: err}
    }
}



const modelnameToObject = async(modelname)=>{
    // look up models available on hosts
    let availableHosts=cluster.filter(h=>{return h.online&&!h.disabled})
    for (const h in availableHosts){
        let host=cluster[h]
        let model=host.models.find(m=>{return m.model_name===modelname})
        if(isObject(model)){
            return {
                model_name: model.model_name,
                base_model: model.base_model,
                model_type: model.model_type,
                description: model.description
            }
        }else{ log('No model with name '+modelname+' on host '+host.name)}
    }
    return {error:'Unable to find online host with model: `'+modelname+'`'}
}

const loranameToObject = async(loraname)=>{
    // look up loras available on hosts
    //if(!loraname){throw('loranameToObject ')}
    let availableHosts=cluster.filter(h=>{return h.online&&!h.disabled})
    for (const h in availableHosts){
        let host=cluster[h]
        let lora=host.lora.find(m=>{return m.model_name===loraname})
        debugLog(lora)
        if(isObject(lora)){
            return {model_name: lora.model_name,base_model: lora.base_model}
        }else{log('Error: No lora with name '+loraname+' on host '+host.name)}
    }
    return {error:'Unable to find online host with lora: `'+loraname+'`'}
}

const controlnetnameToObject = async(controlnetname) => {
    // Look up controlnets available on hosts
    let availableHosts = cluster.filter(host => {
        return host.online && !host.disabled  
    })
    for(let host of availableHosts) {
        let controlnet = host.controlnets.find(c => {return c.model_name === controlnetname})
        if(controlnet) {return {model_name: controlnet.model_name, base_model: controlnet.base_model,model_type: controlnet.model_type}
        } else {log(`Controlnet ${controlnetname} not found on host ${host.name}`)}
    }
    throw `Unable to find online host with controlnet: ${controlnetname}`
}


getHostById = (id)=>{return cluster.find(h=>{h.id===id})}
getHostByName = (name)=>{return cluster.find(h=>{h.name===name})}
getHostByJobId = (id)=>{return cluster.find(h=>{h.jobs.includes(id)})}

cast = async(job)=>{
    // easy mode, submit job, receive results
    const context = {
        job,
        host:null,
        batchId:null,
        sessionId:null,
        images:[]
    }
    try{
        context.host=await findHost(context.job)
        if(context.job.initimg){
            debugLog('Uploading initimg')
            initimgid = getUUID()
            context.job.initimgObject = await uploadInitImage(context.host,context.job.initimg,initimgid)
            if(!context.job.control){context.job.control='i2l'}
        }
        let graph = await buildGraphFromJob(context.job)
        context.batchId = await enqueueBatch(context.host,graph)
        context.sessionId = await pollQueue(context.host,context.batchId)
        let session = await pollSession(context.host,context.sessionId) // returned finished session
        debugLog(context.host.name+' '+context.sessionId+' collecting images ')
        context.images = await getSessionImages(context.host,session)   
        if(context.images?.error){return {error:context.images?.error}}
        if(context.job.initimgObject)deleteImage(context.host,context.job.initimgObject.image_name) // remove uploaded image after use
        let result = {
            job:context.job,
            host:context.host,
            images:context.images,
            session:session
        }
        return result
    }catch(err){
        return {error:err}
    }
}

init().then((r)=>{log(r)}).catch(e=>{log(e)})

module.exports = {
    cluster,
    invoke:{
        init,
        cast,
        jobFromDream,
        jobFromMeta,
        validateJob,
        rawGraphResponse,
        findHost,
        depthMap,
        canny,
        openpose,
        esrgan,
        allUniqueModelsAvailable
    }
}
